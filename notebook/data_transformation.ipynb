{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\NASA-Battery-Life-Prediction\\\\notebook'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\NASA-Battery-Life-Prediction'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class DataTransformationConfig:\n",
    "    root_dir:Path\n",
    "    data_path: Path\n",
    "    seq_length: int = 2\n",
    "    test_size: float = 0.25\n",
    "    random_state: int = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.constants import *\n",
    "from src.utils.common import read_yaml, create_directories, save_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config=self.config.data_transformation\n",
    "        create_directories([config.root_dir])\n",
    "        data_tranformation_config=DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path\n",
    "        )\n",
    "        return data_tranformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import os\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Tuple\n",
    "from src import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "        self.encoders = {}\n",
    "        create_directories([self.config.root_dir])\n",
    "    \n",
    "    def process_start_time(self, time_str):\n",
    "        try:\n",
    "            if isinstance(time_str, str):\n",
    "                time_vals = [float(x) for x in time_str.strip('[]').split()]\n",
    "            else:\n",
    "                time_vals = time_str\n",
    "                \n",
    "            year, month, day, hour, minute, second = map(float, time_vals)\n",
    "            return pd.Timestamp(int(year), int(month), int(day), \n",
    "                              int(hour), int(minute), int(second))\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error processing time: {e}\")\n",
    "            return pd.NaT\n",
    "\n",
    "    def clean_and_encode_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        try:\n",
    "            df = df.copy()\n",
    "            \n",
    "            numeric_cols = ['Capacity', 'Re', 'Rct']\n",
    "            for col in numeric_cols:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            \n",
    "            label_encoder = LabelEncoder()\n",
    "            df['type'] = label_encoder.fit_transform(df['type'])\n",
    "            self.encoders['type_encoder'] = label_encoder\n",
    "            \n",
    "            df['battery_id'] = df['battery_id'].str.replace('B', '').astype(int)\n",
    "            df['start_time'] = df['start_time'].apply(self.process_start_time)\n",
    "            \n",
    "            df['hour'] = df['start_time'].dt.hour\n",
    "            df['day'] = df['start_time'].dt.day\n",
    "            df['month'] = df['start_time'].dt.month\n",
    "            \n",
    "            df['ambient_temperature'] = pd.to_numeric(df['ambient_temperature'], errors='coerce')\n",
    "            \n",
    "            numeric_columns = ['ambient_temperature', 'Capacity', 'Re', 'Rct']\n",
    "            for col in numeric_columns:\n",
    "                if col in df.columns:\n",
    "                    median_val = df[col].median()\n",
    "                    df[col] = df[col].fillna(median_val)\n",
    "                    \n",
    "                    scaler = MinMaxScaler()\n",
    "                    df[col] = scaler.fit_transform(df[[col]])\n",
    "                    self.encoders[f'{col}_scaler'] = scaler\n",
    "            \n",
    "            logger.info(\"Data cleaning and encoding completed successfully\")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in clean_and_encode_data: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def create_sequences(self, data: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        X, y = [], []\n",
    "        for i in range(len(data) - self.config.seq_length):\n",
    "            sequence = data[i:i + self.config.seq_length + 1]\n",
    "            if not np.any(np.isnan(sequence)):\n",
    "                X.append(sequence[:-1])\n",
    "                y.append(sequence[-1])\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    def save_data(self, X: np.ndarray, y: np.ndarray, filename: str) -> pd.DataFrame:\n",
    "        X_2d = X.reshape(X.shape[0], -1)\n",
    "        feature_cols = [f'time_step_{i}' for i in range(X.shape[1])]\n",
    "        \n",
    "        X_df = pd.DataFrame(X_2d, columns=feature_cols)\n",
    "        y_df = pd.DataFrame(y, columns=['target'])\n",
    "        \n",
    "        combined_df = pd.concat([X_df, y_df], axis=1)\n",
    "        save_path = os.path.join(self.config.root_dir, filename)\n",
    "        combined_df.to_csv(save_path, index=False)\n",
    "        \n",
    "        logger.info(f\"Data saved to: {save_path}\")\n",
    "        return combined_df\n",
    "\n",
    "    def train_test_spliting(self):\n",
    "        try:\n",
    "            logger.info(\"Started data transformation\")\n",
    "            \n",
    "            data = pd.read_csv(self.config.data_path)\n",
    "            logger.info(f\"Read data from {self.config.data_path}, shape: {data.shape}\")\n",
    "            \n",
    "            processed_df = self.clean_and_encode_data(data)\n",
    "            capacity_data = processed_df['Capacity'].values\n",
    "            \n",
    "            X, y = self.create_sequences(capacity_data)\n",
    "            \n",
    "            if len(X) == 0:\n",
    "                raise ValueError(\"No valid sequences could be created. Check if there are enough consecutive non-null Capacity values.\")\n",
    "            \n",
    "            logger.info(f\"Created sequences with shape X: {X.shape}, y: {y.shape}\")\n",
    "            \n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, \n",
    "                test_size=self.config.test_size,\n",
    "                random_state=self.config.random_state\n",
    "            )\n",
    "            \n",
    "            train_df = self.save_data(X_train, y_train, \"train.csv\")\n",
    "            test_df = self.save_data(X_test, y_test, \"test.csv\")\n",
    "            \n",
    "            # Save encoders using the imported save_bin function\n",
    "            encoder_path = Path(self.config.root_dir) / \"encoders.joblib\"\n",
    "            joblib.dump(self.encoders, encoder_path)\n",
    "            \n",
    "            logger.info(\"Data transformation completed\")\n",
    "            logger.info(f\"Training set shape: {train_df.shape}\")\n",
    "            logger.info(f\"Test set shape: {test_df.shape}\")\n",
    "            \n",
    "            return train_df, test_df\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in data transformation: {e}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-27 01:54:20,781: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-12-27 01:54:20,794: INFO: common: yaml file: params.yaml loaded successfully]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-27 01:54:20,798: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2024-12-27 01:54:20,801: INFO: common: created directory at: artifacts]\n",
      "[2024-12-27 01:54:20,803: INFO: common: created directory at: artifacts/data_transformation]\n",
      "[2024-12-27 01:54:20,806: INFO: common: created directory at: artifacts/data_transformation]\n",
      "[2024-12-27 01:54:20,808: INFO: 3760138798: Started data transformation]\n",
      "[2024-12-27 01:54:20,897: INFO: 3760138798: Read data from artifacts/data_ingestion/metadata.csv, shape: (7565, 10)]\n",
      "[2024-12-27 01:54:21,086: INFO: 3760138798: Data cleaning and encoding completed successfully]\n",
      "[2024-12-27 01:54:21,130: INFO: 3760138798: Created sequences with shape X: (7563, 2), y: (7563,)]\n",
      "[2024-12-27 01:54:21,253: INFO: 3760138798: Data saved to: artifacts/data_transformation\\train.csv]\n",
      "[2024-12-27 01:54:21,280: INFO: 3760138798: Data saved to: artifacts/data_transformation\\test.csv]\n",
      "[2024-12-27 01:54:21,310: INFO: 3760138798: Data transformation completed]\n",
      "[2024-12-27 01:54:21,311: INFO: 3760138798: Training set shape: (5672, 3)]\n",
      "[2024-12-27 01:54:21,313: INFO: 3760138798: Test set shape: (1891, 3)]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    transformer = DataTransformation(config=data_transformation_config)\n",
    "    train_df, test_df = transformer.train_test_spliting()\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in data transformation: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
